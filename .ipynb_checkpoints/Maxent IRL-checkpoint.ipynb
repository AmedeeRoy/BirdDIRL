{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRL\n",
    "\n",
    "Easy game formalism :\n",
    "\n",
    "- States = x\n",
    "- Action = (&uarr;, &darr;, &rarr;, &larr;)\n",
    "- Reward = r(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridworld provides a basic environment for RL agents to interact with\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    Grid world environment\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid):\n",
    "        \"\"\"\n",
    "            input: grid 2-d list of the grid including the reward and the start position defined as 'x'\n",
    "        \"\"\"\n",
    "        \n",
    "        self.height = len(grid)\n",
    "        self.width = len(grid[0])\n",
    "        self.n_states = self.height*self.width + 1\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                grid[i][j] = str(grid[i][j])\n",
    "        self.grid = grid\n",
    "                \n",
    "        self.neighbors = [(0, 0), (-1, 0), (-1, 1), (0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
    "        self.actions = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.dirs = {0: 'stay', 1: 'n', 2: 'ne', 3: 'e', 4: 'se', 5: 's', 6: 'sw', 7: 'w', 8: 'nw'}\n",
    "            \n",
    "    def show_grid(self):\n",
    "        for i in range(len(self.grid)):\n",
    "            print(self.grid[i])\n",
    "            \n",
    "    def get_grid(self):\n",
    "        return self.grid\n",
    "      \n",
    "    def get_start(self):\n",
    "        f = filter(lambda x: self.grid[x[0]][x[1]] == 'x',\n",
    "                                [(i, j) for i in range(self.height) for j in range(self.width)])\n",
    "        return list(f)[0]\n",
    "    \n",
    "    def get_states(self):                       \n",
    "        return [(i, j) for i in range(self.height) for j in range(self.width)] + [(-1,-1)]\n",
    "    \n",
    "    def get_grid_idx(self):\n",
    "        return np.array([i for i in range(self.n_states-1)]).reshape(self.height,self.width)\n",
    "    \n",
    "    def state2idx(self, state):\n",
    "        \"\"\"\n",
    "        input:\n",
    "          2d state\n",
    "        returns:\n",
    "          1d index\n",
    "        \"\"\"\n",
    "        if state[0] < 0:\n",
    "            return self.n_states-1\n",
    "        else:\n",
    "            return self.get_grid_idx()[state]\n",
    "\n",
    "    def idx2state(self, idx):\n",
    "        \"\"\"\n",
    "        input:\n",
    "          1d idx\n",
    "        returns:\n",
    "          2d state\n",
    "        \"\"\"\n",
    "        if idx == self.height*self.width:\n",
    "            return (-1,-1)\n",
    "        else:\n",
    "            return (idx // self.width, idx % self.width)\n",
    "           \n",
    "    def get_transition_states(self, state, action):\n",
    "        \"\"\"\n",
    "        get all the possible transition states and their probabilities with [action] on [state]\n",
    "        args\n",
    "          state     (y, x)\n",
    "          action    int\n",
    "        returns\n",
    "          new state\n",
    "        \"\"\"\n",
    "        if state == (-1,-1):\n",
    "            return state\n",
    "\n",
    "        inc = self.neighbors[action]\n",
    "        nei_s = (state[0] + inc[0], state[1] + inc[1])\n",
    "        if nei_s[0] >= 0 and nei_s[0] < self.height and nei_s[1] >= 0 and nei_s[1] < self.width:\n",
    "            if nei_s == self.get_start():\n",
    "                nei_s = (-1,-1)\n",
    "            return nei_s\n",
    "        else:\n",
    "            return state\n",
    "        \n",
    "    def get_transition_mat(self):\n",
    "        \"\"\"\n",
    "        get transition dynamics of the gridworld\n",
    "        return:\n",
    "          P_a         NxNxN_ACTIONS transition probabilities matrix - \n",
    "                        P_a[s0, s1, a] is the transition prob of \n",
    "                        landing at state s1 when taking action \n",
    "                        a at state s0\n",
    "        \"\"\"\n",
    "        N_STATES = self.height*self.width+1\n",
    "        N_ACTIONS = len(self.actions)\n",
    "        P_a = np.zeros((N_STATES, N_STATES, N_ACTIONS))\n",
    "        for i in range(N_STATES):\n",
    "            si = self.idx2state(i)\n",
    "            for a in range(N_ACTIONS):\n",
    "                sj = self.get_transition_states(si,a)\n",
    "                j = self.state2idx(sj)\n",
    "                P_a[i, j, a] = 1                \n",
    "        return P_a\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        \"\"\"\n",
    "        returns\n",
    "          the reward on current state\n",
    "        \"\"\"\n",
    "        if state[0]<0:\n",
    "            return 0\n",
    "        elif state == self.get_start():\n",
    "            return 0\n",
    "        else:\n",
    "            return float(self.grid[state[0]][state[1]])\n",
    "    \n",
    "    def get_reward_mat(self):\n",
    "        \"\"\"\n",
    "        Get reward matrix from gridworld\n",
    "        \"\"\"\n",
    "        r_mat = np.zeros((self.n_states,1))\n",
    "        for i in range(self.n_states):\n",
    "                r_mat[i] = self.get_reward(self.idx2state(i))\n",
    "        return r_mat\n",
    "\n",
    "    ##############################################\n",
    "    # Stateful Functions For Model-Free Learning #\n",
    "    ##############################################\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the gridworld for model-free learning.\n",
    "          start_pos     (i,j) pair of the start location\n",
    "        \"\"\"\n",
    "        self.cur_state = self.get_start()\n",
    "         \n",
    "    def get_current_state(self):\n",
    "        return self.cur_state\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        \"\"\"\n",
    "        returns\n",
    "          True when agent come back to initial position (i.e. state = (-1,-1))\n",
    "        \"\"\"\n",
    "        if self.cur_state == (-1,-1):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Step function for the agent to interact with gridworld\n",
    "        args\n",
    "          action        action taken by the agent\n",
    "        returns\n",
    "          current_state current state\n",
    "          action        input action\n",
    "          next_state    next_state\n",
    "          reward        reward on the next state\n",
    "          is_done       True/False - if the agent is already on the terminal states\n",
    "        \"\"\"\n",
    "   \n",
    "        if self.is_terminal():\n",
    "            return self.cur_state, action, self.cur_state, self.get_reward(self.cur_state), True\n",
    "        \n",
    "        last_state = self.cur_state\n",
    "        next_state = self.get_transition_states(last_state, action)\n",
    "        reward = self.get_reward(next_state)\n",
    "        self.cur_state = next_state\n",
    "\n",
    "        return last_state, action, next_state, reward, self.is_terminal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '0', '1', '5', '3', '2']\n",
      "['0', '0', 'x', '10', '2', '1']\n",
      "['0', '0', '3', '4', '1', '2']\n"
     ]
    }
   ],
   "source": [
    "grid = [[0, 0, 1, 5, 3, 2],\n",
    "        [0, 0, 'x', 10, 2, 1],\n",
    "        [0, 0, 3, 4, 1, 2]]\n",
    "\n",
    "g = GridWorld(grid)\n",
    "g.show_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 2), 1, (0, 2), 1.0, False)\n",
      "((0, 2), 2, (0, 2), 1.0, False)\n",
      "((0, 2), 3, (0, 3), 5.0, False)\n",
      "((0, 3), 4, (1, 4), 2.0, False)\n",
      "((1, 4), 5, (2, 4), 1.0, False)\n",
      "((2, 4), 6, (2, 4), 1.0, False)\n",
      "((2, 4), 7, (2, 3), 4.0, False)\n",
      "((2, 3), 8, (-1, -1), 0, True)\n",
      "((-1, -1), 3, (-1, -1), 0, True)\n"
     ]
    }
   ],
   "source": [
    "g.reset()\n",
    "print(g.step(1))\n",
    "print(g.step(2))\n",
    "print(g.step(3))\n",
    "print(g.step(4))\n",
    "print(g.step(5))\n",
    "print(g.step(6))\n",
    "print(g.step(7))\n",
    "print(g.step(8))\n",
    "print(g.step(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P_a, rewards, gamma, error=0.01, deterministic=True):\n",
    "    \"\"\"\n",
    "    static value iteration function. Perhaps the most useful function in this repo\n",
    "\n",
    "    inputs:\n",
    "    P_a         NxNxN_ACTIONS transition probabilities matrix - \n",
    "                          P_a[s0, s1, a] is the transition prob of \n",
    "                          landing at state s1 when taking action \n",
    "                          a at state s0\n",
    "    rewards     Nx1 matrix - rewards for all the states\n",
    "    gamma       float - RL discount\n",
    "    error       float - threshold for a stop\n",
    "\n",
    "    returns:\n",
    "    values    Nx1 matrix - estimated values\n",
    "    policy    Nx1 matrix - policy\n",
    "    \"\"\"\n",
    "    N_STATES, _, N_ACTIONS = np.shape(P_a)\n",
    "\n",
    "    values = np.zeros([N_STATES])\n",
    "\n",
    "    # estimate values\n",
    "    while True:\n",
    "        values_tmp = values.copy()\n",
    "\n",
    "        for s in range(N_STATES-1):\n",
    "            v_s = []\n",
    "            values[s] = max([sum([P_a[s, s1, a]*(rewards[s] + gamma*values_tmp[s1]) for s1 in range(N_STATES)]) for a in range(N_ACTIONS)])\n",
    "\n",
    "        if max([abs(values[s] - values_tmp[s]) for s in range(N_STATES)]) < error:\n",
    "            break\n",
    "\n",
    "    if deterministic:\n",
    "        policy = np.zeros([N_STATES])\n",
    "        for s in range(N_STATES-1):\n",
    "            policy[s] = np.argmax([sum([P_a[s, s1, a]*(rewards[s]+gamma*values[s1]) \n",
    "                                      for s1 in range(N_STATES)]) \n",
    "                                      for a in range(N_ACTIONS)])\n",
    "        return values, policy\n",
    "    else:\n",
    "        # generate stochastic policy\n",
    "        policy = np.zeros([N_STATES, N_ACTIONS])\n",
    "        for s in range(N_STATES-1):\n",
    "            v_s = np.array([sum([P_a[s, s1, a]*(rewards[s] + gamma*values[s1]) for s1 in range(N_STATES)]) for a in range(N_ACTIONS)])\n",
    "            policy[s,:] = np.transpose(v_s/np.sum(v_s))\n",
    "        policy[N_STATES-1,:] = [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        return values, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.10887046e-01 1.10887046e-01 1.10887046e-01 1.11782428e-01\n",
      "  1.12008250e-01 1.10887046e-01 1.10887046e-01 1.10887046e-01\n",
      "  1.10887046e-01]\n",
      " [1.25060791e-01 1.25060791e-01 1.25060791e-01 1.26325299e-01\n",
      "  0.00000000e+00 1.25313438e-01 1.24059049e-01 1.24059049e-01\n",
      "  1.25060791e-01]\n",
      " [1.25060413e-01 1.25060413e-01 1.25060413e-01 1.25565184e-01\n",
      "  1.26196148e-01 1.27467430e-04 1.24059705e-01 1.23809843e-01\n",
      "  1.25060413e-01]\n",
      " [1.24983573e-01 1.24983573e-01 1.24983573e-01 1.24733372e-01\n",
      "  1.24608272e-01 1.25609074e-01 6.31819470e-04 1.24483172e-01\n",
      "  1.24983573e-01]\n",
      " [1.11221147e-01 1.11221147e-01 1.11221147e-01 1.10334773e-01\n",
      "  1.10222999e-01 1.11109372e-01 1.12003570e-01 1.11444697e-01\n",
      "  1.11221147e-01]\n",
      " [1.10937860e-01 1.10937860e-01 1.10937860e-01 1.10937860e-01\n",
      "  1.10937860e-01 1.10825359e-01 1.11717491e-01 1.11829991e-01\n",
      "  1.10937860e-01]\n",
      " [1.10762858e-01 1.10762858e-01 1.11657238e-01 1.11882806e-01\n",
      "  1.11882806e-01 1.10762858e-01 1.10762858e-01 1.10762858e-01\n",
      "  1.10762858e-01]\n",
      " [1.25216944e-01 1.24964493e-01 1.26228026e-01 0.00000000e+00\n",
      "  1.26483028e-01 1.25216944e-01 1.23963521e-01 1.23963521e-01\n",
      "  1.23963521e-01]\n",
      " [0.00000000e+00 1.25122751e-01 1.25628289e-01 1.26260211e-01\n",
      "  1.25501904e-01 1.25375520e-01 1.24120522e-01 1.24120522e-01\n",
      "  1.23870281e-01]\n",
      " [1.25636497e-01 1.25013991e-01 1.24764988e-01 1.24640487e-01\n",
      "  1.24515985e-01 1.24889489e-01 1.24764988e-01 1.25758909e-03\n",
      "  1.24515985e-01]\n",
      " [1.11232832e-01 1.11344844e-01 1.10456583e-01 1.10344570e-01\n",
      "  1.10345690e-01 1.11120819e-01 1.11456857e-01 1.12128934e-01\n",
      "  1.11568870e-01]\n",
      " [1.10800859e-01 1.10913451e-01 1.10800859e-01 1.10800859e-01\n",
      "  1.10800859e-01 1.10801985e-01 1.11581118e-01 1.11693709e-01\n",
      "  1.11806301e-01]\n",
      " [1.10862011e-01 1.10862011e-01 1.11982962e-01 1.11982962e-01\n",
      "  1.10862011e-01 1.10862011e-01 1.10862011e-01 1.10862011e-01\n",
      "  1.10862011e-01]\n",
      " [1.25155019e-01 1.25155019e-01 0.00000000e+00 1.26420476e-01\n",
      "  1.25155019e-01 1.25155019e-01 1.25155019e-01 1.23902215e-01\n",
      "  1.23902215e-01]\n",
      " [1.25138805e-01 3.81096035e-04 1.26019137e-01 1.25264566e-01\n",
      "  1.25138805e-01 1.25138805e-01 1.25138805e-01 1.23889991e-01\n",
      "  1.23889991e-01]\n",
      " [1.24936714e-01 1.25688547e-01 1.24686103e-01 1.24560798e-01\n",
      "  1.24936714e-01 1.24936714e-01 1.24936714e-01 1.24811409e-01\n",
      "  5.06284707e-04]\n",
      " [1.11121701e-01 1.11233829e-01 1.10344655e-01 1.10345776e-01\n",
      "  1.11121701e-01 1.11121701e-01 1.11121701e-01 1.11458084e-01\n",
      "  1.12130851e-01]\n",
      " [1.10925569e-01 1.10924443e-01 1.10925569e-01 1.10925569e-01\n",
      "  1.10925569e-01 1.10925569e-01 1.10925569e-01 1.11704771e-01\n",
      "  1.11817372e-01]\n",
      " [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "values, policy = value_iteration(g.get_transition_mat(), g.get_reward_mat(), 0.99, 0.01, deterministic = False)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_state_visition_freq(P_a, gamma, start_idx, nb_step, policy, deterministic=True):\n",
    "    \"\"\"compute the expected states visition frequency p(s| theta, T) \n",
    "    using dynamic programming\n",
    "    inputs:\n",
    "    P_a     NxNxN_ACTIONS matrix - transition dynamics\n",
    "    gamma   float - discount factor\n",
    "    start_idx   idx of start position\n",
    "    nb_step idx - nb of step to iterate\n",
    "    policy  Nx1 vector - policy\n",
    "\n",
    "    returns:\n",
    "    p       Nx1 vector - state visitation frequencies\n",
    "    \"\"\"\n",
    "    N_STATES, _, N_ACTIONS = np.shape(P_a)\n",
    "\n",
    "    # mu[s, t] is the prob of visiting state s at time t\n",
    "    mu = np.zeros([N_STATES, nb_step]) \n",
    "\n",
    "    mu[start_idx, 0] = 1\n",
    "    \n",
    "    for s in range(N_STATES-1):\n",
    "        for t in range(nb_step-1):\n",
    "            if deterministic:\n",
    "                mu[s, t+1] = sum([mu[pre_s, t]*P_a[pre_s, s, int(policy[pre_s])] for pre_s in range(N_STATES)])\n",
    "            else:\n",
    "                mu[s, t+1] = sum([sum([mu[pre_s, t]*P_a[pre_s, s, a1]*policy[pre_s, a1] for a1 in range(N_ACTIONS)]) for pre_s in range(N_STATES)])\n",
    "\n",
    "    p = np.sum(mu, 1)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.24737488, 0.31129995, 0.32668674, 0.07148539,\n",
       "       0.01910197, 0.05447314, 0.22796654, 1.        , 0.24443861,\n",
       "       0.09021153, 0.03296805, 0.09325064, 0.3335007 , 0.44690467,\n",
       "       0.43454503, 0.16897754, 0.07585859, 0.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svf = compute_state_visition_freq(g.get_transition_mat(), 0.99, g.state2idx(g.get_start()), 10, policy, deterministic = False)\n",
    "svf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create expert trajectories\n",
    "\n",
    "# grid = [[0, 0, 1, 5, 3, 2],\n",
    "#         [0, 0, 'x', 10, 2, 1],\n",
    "#         [0, 0, 3, 4, 1, 2]]\n",
    "\n",
    "g.reset()\n",
    "traj_1 = [g.step(3),\n",
    "          g.step(0),\n",
    "          g.step(0),\n",
    "          g.step(1),\n",
    "          g.step(6)\n",
    "         ]\n",
    "\n",
    "g.reset()\n",
    "traj_2 = [g.step(5),\n",
    "          g.step(3),\n",
    "          g.step(3),\n",
    "          g.step(5),\n",
    "          g.step(1),\n",
    "          g.step(8),\n",
    "          g.step(6)\n",
    "         ]   \n",
    "\n",
    "g.reset()\n",
    "traj_3 = [g.step(5),\n",
    "          g.step(3),\n",
    "          g.step(3),\n",
    "          g.step(1),\n",
    "          g.step(5),\n",
    "          g.step(7),\n",
    "          g.step(7),\n",
    "          g.step(1)\n",
    "         ]  \n",
    "\n",
    "trajs = [traj_1, traj_1, traj_1, traj_1, traj_1, traj_1,\n",
    "         traj_2, traj_2, traj_2, traj_3, traj_3, traj_3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  9.,  0.,  0.],\n",
       "       [ 0.,  0., 12., 18.,  6.,  0.],\n",
       "       [ 0.,  0.,  9.,  9., 12.,  0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = np.zeros((g.height, g.width))\n",
    "for traj in trajs:\n",
    "    for step in traj:\n",
    "        freq[step[0]] += 1\n",
    "        \n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(vals):\n",
    "    \"\"\"\n",
    "    normalize to (0, max_val)\n",
    "    input:\n",
    "    vals: 1d array\n",
    "    \"\"\"\n",
    "    min_val = np.min(vals)\n",
    "    max_val = np.max(vals)\n",
    "    return (vals - min_val) / (max_val - min_val)\n",
    "\n",
    "def maxent_irl(feat_map, P_a, gamma, trajs, lr, n_iters):\n",
    "    \"\"\"\n",
    "    Maximum Entropy Inverse Reinforcement Learning (Maxent IRL)\n",
    "    inputs:\n",
    "    feat_map    NxD matrix - the features for each state\n",
    "    P_a         NxNxN_ACTIONS matrix - P_a[s0, s1, a] is the transition prob of \n",
    "                                       landing at state s1 when taking action \n",
    "                                       a at state s0\n",
    "    gamma       float - RL discount factor\n",
    "    trajs       a list of demonstrations\n",
    "    lr          float - learning rate\n",
    "    n_iters     int - number of optimization steps\n",
    "    returns\n",
    "    rewards     Nx1 vector - recoverred state rewards\n",
    "    \"\"\"\n",
    "    N_STATES, _, N_ACTIONS = np.shape(P_a)\n",
    "\n",
    "    # init parameters\n",
    "    theta = np.random.uniform(size=(feat_map.shape[1]))\n",
    "\n",
    "    # calc feature expectations\n",
    "    feat_exp = np.zeros([feat_map.shape[1]])\n",
    "    for episode in trajs:\n",
    "        for step in episode:\n",
    "            feat_exp += feat_map[g.state2idx(step[0]),:]\n",
    "    feat_exp = feat_exp/len(trajs)\n",
    "\n",
    "    # training\n",
    "    for iteration in range(n_iters):\n",
    "\n",
    "        if iteration % (n_iters/20) == 0:\n",
    "            print('iteration: {}/{}'.format(iteration, n_iters))\n",
    "\n",
    "        # compute reward function\n",
    "        rewards = np.dot(feat_map, theta)\n",
    "\n",
    "        # compute policy\n",
    "        _, policy = value_iteration(P_a, rewards, gamma, error=0.01, deterministic = False)\n",
    "\n",
    "        # compute state visition frequences\n",
    "        svf = compute_state_visition_freq(P_a, gamma, g.state2idx(g.get_start()), 10, policy, deterministic = False)\n",
    "        \n",
    "        # compute gradients\n",
    "        grad = feat_exp - feat_map.T.dot(svf)\n",
    "\n",
    "        # update params\n",
    "        theta += lr * grad\n",
    "\n",
    "    rewards = np.dot(feat_map, theta)\n",
    "    return normalize(rewards), policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [1, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 2],\n",
       "       [0, 3],\n",
       "       [1, 2],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [0, 1],\n",
       "       [0, 2],\n",
       "       [0, 3],\n",
       "       [1, 2],\n",
       "       [1, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 2],\n",
       "       [0, 3],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## feat_map\n",
    "\n",
    "map_mask = [[1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0]]\n",
    "\n",
    "map_dist = [[2, 1, 1, 1, 2, 3],\n",
    "            [2, 1, 0, 1, 2, 3],\n",
    "            [2, 1, 1, 1, 2, 3]]\n",
    "\n",
    "map_gradv = [[1, 1, 1, 1, 1, 1],\n",
    "            [2, 2, 2, 2, 2, 2],\n",
    "            [3, 3, 3, 3, 3, 3]]\n",
    "\n",
    "map_gradh = [[1, 2, 3, 4, 5, 6],\n",
    "            [1, 2, 3, 4, 5, 6],\n",
    "            [1, 3, 3, 4, 5, 6]]\n",
    "\n",
    "\n",
    "feature = np.array([map_mask, map_dist]).reshape(2,18).T\n",
    "\n",
    "newrow = feature[g.state2idx(g.get_start()),:]\n",
    "feature = np.vstack([feature, newrow])\n",
    "\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0/10\n",
      "iteration: 1/10\n",
      "iteration: 2/10\n",
      "iteration: 3/10\n",
      "iteration: 4/10\n",
      "iteration: 5/10\n",
      "iteration: 6/10\n",
      "iteration: 7/10\n",
      "iteration: 8/10\n",
      "iteration: 9/10\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards, policy = maxent_irl(feature, g.get_transition_mat(), 0.99, trajs, 0.1, 10)\n",
    "\n",
    "# Map\n",
    "plt.imshow(rewards[range(g.n_states-1)].reshape((g.height,g.width)));\n",
    "plt.title('Rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_demonstrations(g, policy, n_trajs=1, len_traj=20):\n",
    "    \"\"\"gatheres expert demonstrations\n",
    "    inputs:\n",
    "    gw          Gridworld - the environment\n",
    "    policy      Nx1 matrix\n",
    "    n_trajs     int - number of trajectories to generate\n",
    "    rand_start  bool - randomly picking start position or not\n",
    "    start_pos   2x1 list - set start position, default [0,0]\n",
    "    returns:\n",
    "    trajs       a list of trajectories - each element in the list is a list of Steps representing an episode\n",
    "    \"\"\"\n",
    "\n",
    "    trajs = []\n",
    "    for i in range(n_trajs):\n",
    "        episode = []\n",
    "        g.reset()\n",
    "        act = np.random.choice(g.n_actions, p= policy[g.state2idx(g.get_start()),:])\n",
    "        step = g.step(act)\n",
    "        episode.append(step)\n",
    "        # while not is_done:\n",
    "        for _ in range(len_traj):\n",
    "            act = np.random.choice(g.n_actions, p= policy[g.state2idx(step[2]),:])\n",
    "            step = g.step(act)\n",
    "            episode.append(step)\n",
    "            if step[4]:\n",
    "                break\n",
    "        trajs.append(episode)\n",
    "    return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3., 12., 11.,  3.,  1.,  0.],\n",
       "       [ 6.,  9., 10.,  4.,  2.,  1.],\n",
       "       [ 8.,  7., 11., 18.,  4.,  0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajs = generate_demonstrations(g, policy, 10, 10)\n",
    "\n",
    "freq = np.zeros((g.height, g.width))\n",
    "for traj in trajs:\n",
    "    for step in traj:\n",
    "        freq[step[0]] += 1\n",
    "        \n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

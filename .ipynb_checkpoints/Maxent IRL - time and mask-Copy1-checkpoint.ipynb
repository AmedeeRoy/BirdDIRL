{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRL\n",
    "\n",
    "Easy game formalism :\n",
    "\n",
    "- States = (x, t)\n",
    "- Action = (&uarr;, &darr;, &rarr;, &larr;)\n",
    "- Reward = r(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mdptoolbox\n",
    "\n",
    "FLOAT_MAX = 1e30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridworld provides a basic environment for RL agents to interact with\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    Grid world environment\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, length_max, height, width, start_pos):\n",
    "        \"\"\"\n",
    "            input: \n",
    "            height - idx : height of the spatial grid\n",
    "            width - idx : width of the spatial grid\n",
    "            length - idx : temporal length of a trip\n",
    "            \n",
    "            start_pos 2-tuple : coordinates within the state_space (height x width)\n",
    "            \n",
    "        \"\"\"\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.length_max = length_max\n",
    "        \n",
    "        self.start = (0, start_pos[0], start_pos[1])\n",
    "        self.end = (length_max-1, start_pos[0], start_pos[1])\n",
    "        \n",
    "        self.n_states = self.height*self.width*self.length_max\n",
    "        \n",
    "        self.actions = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.neighbors = [(0, 0),(-1, 0),(-1, 1),(0, 1),(1, 1),(1, 0),(1, -1),(0, -1),(-1, -1)]\n",
    "        self.dirs = {0: 'stay', 1: 'n', 2: 'ne', 3: 'e', 4: 'se', 5: 's', 6: 'sw', 7: 'w', 8: 'nw'}\n",
    "    \n",
    "    def get_grid_idx(self):\n",
    "        return np.array(range(self.n_states)).reshape((self.length_max, self.height, self.width))\n",
    "    \n",
    "    def get_list_state(self):\n",
    "        return [(i,j,k) for i in range(self.length_max) for j in range(self.height) for k in range(self.width)]\n",
    "    \n",
    "    def state2idx(self, state):\n",
    "        \"\"\"\n",
    "        input:\n",
    "          2d state\n",
    "        returns:\n",
    "          1d index\n",
    "        \"\"\"\n",
    "        return self.get_grid_idx()[state]\n",
    "\n",
    "    def idx2state(self, idx):\n",
    "        \"\"\"\n",
    "        input:\n",
    "          1d idx\n",
    "        returns:\n",
    "          2d state\n",
    "        \"\"\"\n",
    "        return self.get_list_state()[idx]\n",
    "           \n",
    "    def get_next_state(self, state, action):\n",
    "        \"\"\"\n",
    "        get next state with [action] on [state]\n",
    "        args\n",
    "          state     (z, y, x)\n",
    "          action    int\n",
    "        returns\n",
    "          new state\n",
    "        \"\"\"\n",
    "        if state[0] >= self.length_max-1:\n",
    "            return state\n",
    "        else :\n",
    "            inc = self.neighbors[action]\n",
    "            nei_s = (state[1] + inc[0], state[2] + inc[1])\n",
    "            if nei_s[0] >= 0 and nei_s[0] < self.height and nei_s[1] >= 0 and nei_s[1] < self.width:\n",
    "                next_state = (state[0] + 1, nei_s[0], nei_s[1])\n",
    "            else:\n",
    "                next_state = (state[0] + 1, state[1], state[2])\n",
    "            return next_state\n",
    "\n",
    "    def get_list_previous_state(self, state):\n",
    "        \"\"\"\n",
    "        args\n",
    "          state     (z, y, x)\n",
    "        returns\n",
    "          tuple\n",
    "              - previous state (z, y, x)\n",
    "              - associated action int\n",
    "        \"\"\"\n",
    "        previous = []\n",
    "        for a in self.actions:\n",
    "            inc = self.neighbors[a]\n",
    "            nei_s = (state[1] - inc[0], state[2] - inc[1])\n",
    "\n",
    "            if nei_s[0] >= 0 and nei_s[0] < self.height and nei_s[1] >= 0 and nei_s[1] < self.width:\n",
    "                previous_state = (state[0] - 1, nei_s[0], nei_s[1])\n",
    "                previous.append((previous_state,a))\n",
    "        return previous\n",
    "\n",
    "    def get_transition_mat(self):\n",
    "        \"\"\"\n",
    "        get transition dynamics of the gridworld\n",
    "        return:\n",
    "          P_a         NxNxN_ACTIONS transition probabilities matrix - \n",
    "                        P_a[s0, s1, a] is the transition prob of \n",
    "                        landing at state s1 when taking action \n",
    "                        a at state s0\n",
    "        \"\"\"\n",
    "        P_a = np.zeros((self.n_states, self.n_states, self.n_actions))\n",
    "        \n",
    "        for i in range(self.n_states):\n",
    "            si = self.idx2state(i)\n",
    "            for a in range(self.n_actions):\n",
    "                sj = self.get_next_state(si,a)\n",
    "                j = self.state2idx(sj)\n",
    "                P_a[i, j, a] = 1                \n",
    "        return P_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = GridWorld(5,3,4,(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]],\n",
       "\n",
       "       [[12, 13, 14, 15],\n",
       "        [16, 17, 18, 19],\n",
       "        [20, 21, 22, 23]],\n",
       "\n",
       "       [[24, 25, 26, 27],\n",
       "        [28, 29, 30, 31],\n",
       "        [32, 33, 34, 35]],\n",
       "\n",
       "       [[36, 37, 38, 39],\n",
       "        [40, 41, 42, 43],\n",
       "        [44, 45, 46, 47]],\n",
       "\n",
       "       [[48, 49, 50, 51],\n",
       "        [52, 53, 54, 55],\n",
       "        [56, 57, 58, 59]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.get_grid_idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P_a, rewards, error=0.01, max_iter=100):\n",
    "    \"\"\"\n",
    "    static value iteration function. Perhaps the most useful function in this repo\n",
    "\n",
    "    inputs:\n",
    "    P_a         NxNxN_ACTIONS transition probabilities matrix - \n",
    "                          P_a[s0, s1, a] is the transition prob of \n",
    "                          landing at state s1 when taking action \n",
    "                          a at state s0\n",
    "    rewards     Nx1 matrix - rewards for all the states\n",
    "    gamma       float - RL discount\n",
    "    error       float - threshold for a stop\n",
    "\n",
    "    returns:\n",
    "    values    Nx1 matrix - estimated values\n",
    "    policy    Nx1 matrix - policy\n",
    "    \"\"\"\n",
    "    N_STATES, _, N_ACTIONS = np.shape(P_a)\n",
    "    n = 0 \n",
    "    values = np.ones([N_STATES])* -FLOAT_MAX\n",
    "    qvalues = np.ones((N_STATES, N_ACTIONS))* -FLOAT_MAX\n",
    "    policy = np.zeros((N_STATES, N_ACTIONS))\n",
    "        \n",
    "    # estimate values\n",
    "    while True:\n",
    "        values_tmp = values.copy()\n",
    "        values[g.state2idx(g.end)] = 0 # goal\n",
    "        \n",
    "        for s in range(N_STATES):\n",
    "            qvalues[s] = [sum([P_a[s, s1, a]*(rewards[s] + values[s1]) for s1 in range(N_STATES)]) for a in range(N_ACTIONS)]\n",
    "            \n",
    "            softmax = max(qvalues[s]) + np.log(1.0 + np.exp(min(qvalues[s]) - max(qvalues[s]))) \n",
    "            values[s] = rewards[s] + softmax\n",
    "            \n",
    "            policy[s,:] = np.exp(qvalues[s]-values[s])/sum(np.exp(qvalues[s]-values[s]))\n",
    "            \n",
    "        if max([abs(values[s] - values_tmp[s]) for s in range(N_STATES)]) < error:\n",
    "            break\n",
    "        n += 1\n",
    "        # max iteration\n",
    "        if n > max_iter:\n",
    "            print(\"    WARNING: max number of iterations\", max_iter)\n",
    "            break    \n",
    "    \n",
    "    return values, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-8.07499498e-01, -1.31979756e+00, -1.51938007e+00,\n",
       "         -2.71758052e+00],\n",
       "        [-1.37244684e+00, -2.02438682e+00, -8.85650639e-01,\n",
       "         -1.93108615e+00],\n",
       "        [-1.27995375e+00, -2.28007957e+00, -1.19345224e+00,\n",
       "         -7.22387827e-01]],\n",
       "\n",
       "       [[-1.91239242e+00, -1.84636118e+00, -1.27638868e+00,\n",
       "         -2.43415945e+00],\n",
       "        [-9.33545781e-01, -1.68428009e+00, -1.44575251e+00,\n",
       "         -1.71327529e+00],\n",
       "        [-7.30168409e-01, -1.32552403e+00, -7.02159848e-01,\n",
       "         -1.23473435e+00]],\n",
       "\n",
       "       [[-2.04135866e+00, -2.87401779e-01, -1.35690256e+00,\n",
       "         -1.96816467e+00],\n",
       "        [-8.51298506e-01, -1.83956088e-01, -7.66555465e-01,\n",
       "         -1.40036538e+00],\n",
       "        [-1.99426980e+00, -1.86317841e+00, -1.61940751e+00,\n",
       "         -8.06033337e-01]],\n",
       "\n",
       "       [[-1.58363034e+00, -2.62951958e-01, -1.92899683e+00,\n",
       "         -1.00000000e+30],\n",
       "        [-1.88164768e+00, -5.91086504e-01, -1.44059953e+00,\n",
       "         -1.00000000e+30],\n",
       "        [-1.72478008e+00, -1.92085177e+00, -7.99857918e-01,\n",
       "         -1.00000000e+30]],\n",
       "\n",
       "       [[-1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30],\n",
       "        [-1.00000000e+30,  1.36240578e-01, -1.00000000e+30,\n",
       "         -1.00000000e+30],\n",
       "        [-1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = g.get_transition_mat()\n",
    "R = -np.random.random((g.n_states, 1))\n",
    "\n",
    "values, policy = value_iteration(P, R, 0.01, 100)\n",
    "\n",
    "values.reshape((g.length_max, g.height, g.width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09102601, 0.09102601, 0.09102601, 0.09723945, 0.11434927,\n",
       "        0.24225524, 0.09102601, 0.09102601, 0.09102601],\n",
       "       [0.08428622, 0.08428622, 0.08428622, 0.14903645, 0.12581682,\n",
       "        0.09911685, 0.20998452, 0.07890047, 0.08428622],\n",
       "       [0.14214592, 0.14214592, 0.14214592, 0.04466023, 0.09183262,\n",
       "        0.11999983, 0.0945343 , 0.08038934, 0.14214592],\n",
       "       [0.07180797, 0.07180797, 0.07180797, 0.07180797, 0.07180797,\n",
       "        0.14765517, 0.19294446, 0.22855256, 0.07180797],\n",
       "       [0.13985212, 0.05254862, 0.0561356 , 0.06601297, 0.0945007 ,\n",
       "        0.17139364, 0.13985212, 0.13985212, 0.13985212],\n",
       "       [0.07024442, 0.0597339 , 0.10562259, 0.08916677, 0.18756043,\n",
       "        0.10055822, 0.18238001, 0.14881667, 0.055917  ],\n",
       "       [0.10815651, 0.12811691, 0.04025251, 0.08276925, 0.13356624,\n",
       "        0.22750495, 0.12197399, 0.08520429, 0.07245536],\n",
       "       [0.08544592, 0.04155424, 0.08544592, 0.08544592, 0.08544592,\n",
       "        0.13788563, 0.23486222, 0.11165418, 0.13226007],\n",
       "       [0.1289908 , 0.10525266, 0.04968134, 0.0711212 , 0.1289908 ,\n",
       "        0.1289908 , 0.1289908 , 0.1289908 , 0.1289908 ],\n",
       "       [0.09307489, 0.06501697, 0.08253116, 0.17360257, 0.09307489,\n",
       "        0.09307489, 0.09307489, 0.16880768, 0.13774205],\n",
       "       [0.15780434, 0.07502064, 0.05741126, 0.0926456 , 0.15780434,\n",
       "        0.15780434, 0.15780434, 0.08460486, 0.05910028],\n",
       "       [0.10949595, 0.0678532 , 0.10949595, 0.10949595, 0.10949595,\n",
       "        0.10949595, 0.10949595, 0.18650574, 0.08866537],\n",
       "       [0.04657278, 0.04657278, 0.04657278, 0.26907041, 0.2983952 ,\n",
       "        0.15309771, 0.04657278, 0.04657278, 0.04657278],\n",
       "       [0.14676642, 0.14676642, 0.14676642, 0.05036727, 0.09089351,\n",
       "        0.16276184, 0.08350826, 0.02540346, 0.14676642],\n",
       "       [0.07434849, 0.07434849, 0.07434849, 0.04034636, 0.07118631,\n",
       "        0.13417037, 0.24025714, 0.21664587, 0.07434849],\n",
       "       [0.07732397, 0.07732397, 0.07732397, 0.07732397, 0.07732397,\n",
       "        0.13642887, 0.25713809, 0.14248919, 0.07732397],\n",
       "       [0.11503288, 0.03499334, 0.20217118, 0.22420492, 0.04181847,\n",
       "        0.03668055, 0.11503288, 0.11503288, 0.11503288],\n",
       "       [0.24832991, 0.22392529, 0.07684663, 0.13867856, 0.05910448,\n",
       "        0.04631824, 0.04062747, 0.1274107 , 0.03875871],\n",
       "       [0.13311517, 0.07376376, 0.04002905, 0.07062646, 0.12796245,\n",
       "        0.05673337, 0.04446008, 0.23836762, 0.21494204],\n",
       "       [0.09890142, 0.05605449, 0.09890142, 0.09890142, 0.09890142,\n",
       "        0.1791916 , 0.07944631, 0.18640719, 0.10329473],\n",
       "       [0.06101836, 0.19135802, 0.37296648, 0.06956532, 0.06101836,\n",
       "        0.06101836, 0.06101836, 0.06101836, 0.06101836],\n",
       "       [0.05793951, 0.31063601, 0.17347309, 0.07393382, 0.05793951,\n",
       "        0.05793951, 0.05793951, 0.05082092, 0.15937811],\n",
       "       [0.06742212, 0.15819449, 0.0839327 , 0.15207098, 0.06742212,\n",
       "        0.06742212, 0.06742212, 0.0528365 , 0.28327683],\n",
       "       [0.12444686, 0.0686861 , 0.12444686, 0.12444686, 0.12444686,\n",
       "        0.12444686, 0.12444686, 0.05517471, 0.12945802],\n",
       "       [0.07583606, 0.07583606, 0.07583606, 0.28407897, 0.20461245,\n",
       "        0.05629223, 0.07583606, 0.07583606, 0.07583606],\n",
       "       [0.17598282, 0.17598282, 0.17598282, 0.03325954, 0.05420314,\n",
       "        0.12675446, 0.03487223, 0.04697934, 0.17598282],\n",
       "       [0.06787957, 0.06787957, 0.06787957, 0.        , 0.        ,\n",
       "        0.11062349, 0.25869389, 0.35916433, 0.06787957],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.6197288 , 0.3802712 , 0.        ],\n",
       "       [0.06188153, 0.08336588, 0.3122854 , 0.22492859, 0.05950246,\n",
       "        0.07239154, 0.06188153, 0.06188153, 0.06188153],\n",
       "       [0.19523228, 0.27105577, 0.05122766, 0.08348584, 0.15844676,\n",
       "        0.05164662, 0.06283401, 0.05371159, 0.07235946],\n",
       "       [0.1029298 , 0.06315865, 0.        , 0.        , 0.        ,\n",
       "        0.19534922, 0.06367519, 0.24070214, 0.33418501],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.5404783 , 0.28477884, 0.17474287],\n",
       "       [0.09273098, 0.07926804, 0.28812552, 0.07622054, 0.09273098,\n",
       "        0.09273098, 0.09273098, 0.09273098, 0.09273098],\n",
       "       [0.06792954, 0.25678425, 0.10980688, 0.20840115, 0.06792954,\n",
       "        0.06792954, 0.06792954, 0.08264403, 0.07064554],\n",
       "       [0.16433798, 0.08658993, 0.        , 0.        , 0.16433798,\n",
       "        0.16433798, 0.16433798, 0.0535669 , 0.20249123],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.65492108, 0.34507892],\n",
       "       [0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        ],\n",
       "       [0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.11111111, 0.11111111, 0.11111111, 0.11111111],\n",
       "       [0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        ],\n",
       "       [0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.11111111, 0.11111111, 0.11111111, 0.11111111],\n",
       "       [0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        ],\n",
       "       [0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.11111111, 0.11111111, 0.11111111, 0.11111111],\n",
       "       [0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.11111111, 0.11111111, 0.11111111, 0.11111111],\n",
       "       [0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.11111111, 0.11111111, 0.11111111, 0.11111111],\n",
       "       [0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.11111111, 0.11111111, 0.11111111, 0.11111111],\n",
       "       [0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.11111111, 0.11111111, 0.11111111, 0.11111111],\n",
       "       [0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.11111111, 0.11111111, 0.11111111, 0.11111111],\n",
       "       [0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.11111111, 0.11111111, 0.11111111, 0.11111111],\n",
       "       [0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.11111111, 0.11111111, 0.11111111, 0.11111111],\n",
       "       [0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.11111111, 0.11111111, 0.11111111, 0.11111111],\n",
       "       [0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.11111111, 0.11111111, 0.11111111, 0.11111111],\n",
       "       [0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.11111111, 0.11111111, 0.11111111, 0.11111111],\n",
       "       [0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.11111111, 0.11111111, 0.11111111, 0.11111111],\n",
       "       [0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.11111111, 0.11111111, 0.11111111, 0.11111111]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_state_visition_freq(P_a, start_idx, nb_step, policy):\n",
    "    \"\"\"compute the expected states visition frequency p(s| theta, T) \n",
    "    using dynamic programming\n",
    "    inputs:\n",
    "    P_a     NxNxN_ACTIONS matrix - transition dynamics\n",
    "    gamma   float - discount factor\n",
    "    start_idx   idx of start position\n",
    "    nb_step idx - nb of step to iterate\n",
    "    policy  Nx1 vector - policy\n",
    "\n",
    "    returns:\n",
    "    p       Nx1 vector - state visitation frequencies\n",
    "    \"\"\"\n",
    "    N_STATES, _, N_ACTIONS = np.shape(P_a)\n",
    "\n",
    "    # mu[s, t] is the prob of visiting state s at time t\n",
    "    mu = np.zeros([N_STATES, nb_step]) \n",
    "\n",
    "    mu[start_idx, 0] = 1\n",
    "    for s in range(N_STATES):\n",
    "        for t in range(nb_step-1):\n",
    "            mu[s, t+1] = sum([sum([mu[pre_s, t]*P_a[pre_s, s, a1]*policy[pre_s, a1] for a1 in range(N_ACTIONS)]) for pre_s in range(N_STATES)])\n",
    "\n",
    "    p = np.sum(mu, 1)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.055917  , 0.0597339 , 0.10562259, 0.        ],\n",
       "        [0.14881667, 0.07024442, 0.08916677, 0.        ],\n",
       "        [0.18238001, 0.10055822, 0.18756043, 0.        ]],\n",
       "\n",
       "       [[0.02507288, 0.13797766, 0.04639547, 0.00783075],\n",
       "        [0.14190083, 0.27623819, 0.08832686, 0.02955887],\n",
       "        [0.08019417, 0.05934378, 0.06722803, 0.03993249]],\n",
       "\n",
       "       [[0.04970881, 0.2696201 , 0.04505895, 0.        ],\n",
       "        [0.06673356, 0.19368799, 0.08415286, 0.        ],\n",
       "        [0.07715289, 0.05417287, 0.15971198, 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svf = compute_state_visition_freq(P, g.state2idx((0,1,1)), g.length_max, policy)\n",
    "svf.reshape((g.length_max, g.height, g.width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]],\n",
       "\n",
       "       [[12, 13, 14, 15],\n",
       "        [16, 17, 18, 19],\n",
       "        [20, 21, 22, 23]],\n",
       "\n",
       "       [[24, 25, 26, 27],\n",
       "        [28, 29, 30, 31],\n",
       "        [32, 33, 34, 35]],\n",
       "\n",
       "       [[36, 37, 38, 39],\n",
       "        [40, 41, 42, 43],\n",
       "        [44, 45, 46, 47]],\n",
       "\n",
       "       [[48, 49, 50, 51],\n",
       "        [52, 53, 54, 55],\n",
       "        [56, 57, 58, 59]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.get_grid_idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.],\n",
       "       [ 0., 12.,  4.,  0.],\n",
       "       [ 0.,  7.,  7.,  0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create expert trajectories\n",
    "\n",
    "traj_1 = [5, 18, 33, 46, 53]\n",
    "traj_2 = [5, 22, 30, 45, 53]\n",
    "traj_3 = [5, 21, 34, 45, 53]\n",
    "traj_4 = [5, 21, 33, 45, 53]\n",
    "traj_5 = [5, 22, 34, 42, 53]\n",
    "traj_6 = [5, 22, 34, 42, 53]\n",
    "\n",
    "trajs = [traj_1, traj_2, traj_3, traj_4, traj_5, traj_6]\n",
    "\n",
    "\n",
    "freq = np.zeros((3,4))\n",
    "for traj in trajs:\n",
    "    for idx in traj:\n",
    "        state = g.idx2state(idx)\n",
    "        freq[state[1], state[2]] += 1\n",
    "        \n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feat_map\n",
    "\n",
    "map_mask = [[-1, -1, -1, 0],\n",
    "            [-1, 0, 0, 0],\n",
    "            [-1, 0, 0, 0]]\n",
    "\n",
    "map_dist = [[-0.5, -0.5, -0.5, -1],\n",
    "            [-0.5, 0, -0.5, -1],\n",
    "            [-0.5, -0.5, -0.5, -1]]\n",
    "\n",
    "map_gradv = [[0, 0, 0, 0],\n",
    "            [-0.5, -0.5, -0.5, -0.5],\n",
    "            [-1, -1, -1, -1]]\n",
    "\n",
    "map_gradh = [[0, -0.3, -0.6, -0.9],\n",
    "            [0, -0.3, -0.6, -0.9],\n",
    "            [0, -0.3, -0.6, -0.9]]\n",
    "\n",
    "map_const = [[0, 0, 0, 0],\n",
    "            [0, 0, 0, 0],\n",
    "            [0, 0, 0, 0]]\n",
    "\n",
    "\n",
    "\n",
    "feat_map = np.array([map_mask, map_dist, map_gradv, map_gradh, map_const]).reshape(5,12).T\n",
    "for t in range(g.length_max-1):\n",
    "    feature =  np.array([map_mask, map_dist, map_gradv, map_gradh, map_const]).reshape(5,12).T\n",
    "    feature[:,4] -= 0.1*(t+1)\n",
    "    \n",
    "    feat_map = np.vstack([feat_map, feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1. , -0.5,  0. ,  0. ,  0. ],\n",
       "       [-1. , -0.5,  0. , -0.3,  0. ],\n",
       "       [-1. , -0.5,  0. , -0.6,  0. ],\n",
       "       [ 0. , -1. ,  0. , -0.9,  0. ],\n",
       "       [-1. , -0.5, -0.5,  0. ,  0. ],\n",
       "       [ 0. ,  0. , -0.5, -0.3,  0. ],\n",
       "       [ 0. , -0.5, -0.5, -0.6,  0. ],\n",
       "       [ 0. , -1. , -0.5, -0.9,  0. ],\n",
       "       [-1. , -0.5, -1. ,  0. ,  0. ],\n",
       "       [ 0. , -0.5, -1. , -0.3,  0. ],\n",
       "       [ 0. , -0.5, -1. , -0.6,  0. ],\n",
       "       [ 0. , -1. , -1. , -0.9,  0. ],\n",
       "       [-1. , -0.5,  0. ,  0. , -0.1],\n",
       "       [-1. , -0.5,  0. , -0.3, -0.1],\n",
       "       [-1. , -0.5,  0. , -0.6, -0.1],\n",
       "       [ 0. , -1. ,  0. , -0.9, -0.1],\n",
       "       [-1. , -0.5, -0.5,  0. , -0.1],\n",
       "       [ 0. ,  0. , -0.5, -0.3, -0.1],\n",
       "       [ 0. , -0.5, -0.5, -0.6, -0.1],\n",
       "       [ 0. , -1. , -0.5, -0.9, -0.1],\n",
       "       [-1. , -0.5, -1. ,  0. , -0.1],\n",
       "       [ 0. , -0.5, -1. , -0.3, -0.1],\n",
       "       [ 0. , -0.5, -1. , -0.6, -0.1],\n",
       "       [ 0. , -1. , -1. , -0.9, -0.1],\n",
       "       [-1. , -0.5,  0. ,  0. , -0.2],\n",
       "       [-1. , -0.5,  0. , -0.3, -0.2],\n",
       "       [-1. , -0.5,  0. , -0.6, -0.2],\n",
       "       [ 0. , -1. ,  0. , -0.9, -0.2],\n",
       "       [-1. , -0.5, -0.5,  0. , -0.2],\n",
       "       [ 0. ,  0. , -0.5, -0.3, -0.2],\n",
       "       [ 0. , -0.5, -0.5, -0.6, -0.2],\n",
       "       [ 0. , -1. , -0.5, -0.9, -0.2],\n",
       "       [-1. , -0.5, -1. ,  0. , -0.2],\n",
       "       [ 0. , -0.5, -1. , -0.3, -0.2],\n",
       "       [ 0. , -0.5, -1. , -0.6, -0.2],\n",
       "       [ 0. , -1. , -1. , -0.9, -0.2],\n",
       "       [-1. , -0.5,  0. ,  0. , -0.3],\n",
       "       [-1. , -0.5,  0. , -0.3, -0.3],\n",
       "       [-1. , -0.5,  0. , -0.6, -0.3],\n",
       "       [ 0. , -1. ,  0. , -0.9, -0.3],\n",
       "       [-1. , -0.5, -0.5,  0. , -0.3],\n",
       "       [ 0. ,  0. , -0.5, -0.3, -0.3],\n",
       "       [ 0. , -0.5, -0.5, -0.6, -0.3],\n",
       "       [ 0. , -1. , -0.5, -0.9, -0.3],\n",
       "       [-1. , -0.5, -1. ,  0. , -0.3],\n",
       "       [ 0. , -0.5, -1. , -0.3, -0.3],\n",
       "       [ 0. , -0.5, -1. , -0.6, -0.3],\n",
       "       [ 0. , -1. , -1. , -0.9, -0.3],\n",
       "       [-1. , -0.5,  0. ,  0. , -0.4],\n",
       "       [-1. , -0.5,  0. , -0.3, -0.4],\n",
       "       [-1. , -0.5,  0. , -0.6, -0.4],\n",
       "       [ 0. , -1. ,  0. , -0.9, -0.4],\n",
       "       [-1. , -0.5, -0.5,  0. , -0.4],\n",
       "       [ 0. ,  0. , -0.5, -0.3, -0.4],\n",
       "       [ 0. , -0.5, -0.5, -0.6, -0.4],\n",
       "       [ 0. , -1. , -0.5, -0.9, -0.4],\n",
       "       [-1. , -0.5, -1. ,  0. , -0.4],\n",
       "       [ 0. , -0.5, -1. , -0.3, -0.4],\n",
       "       [ 0. , -0.5, -1. , -0.6, -0.4],\n",
       "       [ 0. , -1. , -1. , -0.9, -0.4]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxent_irl(feat_map, P_a, trajs, lr, error, max_iter):\n",
    "    \"\"\"\n",
    "    Maximum Entropy Inverse Reinforcement Learning (Maxent IRL)\n",
    "    inputs:\n",
    "    feat_map    NxD matrix - the features for each state\n",
    "    P_a         NxNxN_ACTIONS matrix - P_a[s0, s1, a] is the transition prob of \n",
    "                                       landing at state s1 when taking action \n",
    "                                       a at state s0\n",
    "    gamma       float - RL discount factor\n",
    "    trajs       a list of demonstrations\n",
    "    lr          float - learning rate\n",
    "    n_iters     int - number of optimization steps\n",
    "    returns\n",
    "    rewards     Nx1 vector - recoverred state rewards\n",
    "    \"\"\"\n",
    "    N_STATES, _, N_ACTIONS = np.shape(P_a)\n",
    "\n",
    "    # init parameters\n",
    "    theta = np.random.uniform(size=(feat_map.shape[1]))\n",
    "    \n",
    "    # calc feature expectations\n",
    "    feat_exp = np.zeros([feat_map.shape[1]])\n",
    "    for episode in trajs:\n",
    "        for step in episode:\n",
    "            feat_exp += feat_map[step,:]\n",
    "    feat_exp = feat_exp/len(trajs)\n",
    "\n",
    "    n = 0\n",
    "    error_history = []\n",
    "    # training\n",
    "    while True:\n",
    "        n += 1\n",
    "        if n % (max_iter/20) == 0:\n",
    "            print('iteration: {}/{}'.format(n, max_iter))\n",
    "\n",
    "        # compute reward function\n",
    "        rewards = np.dot(feat_map, theta)\n",
    "\n",
    "        # compute policy\n",
    "        _, policy = value_iteration(P_a, rewards, error=0.01, max_iter=100)\n",
    "\n",
    "        # compute state visition frequences\n",
    "        svf = compute_state_visition_freq(P_a, g.state2idx((0,1,1)), g.length_max, policy)\n",
    "        \n",
    "        # compute gradients\n",
    "        grad = feat_exp - feat_map.T.dot(svf)\n",
    "                \n",
    "        # update params\n",
    "        theta += lr * grad\n",
    "       \n",
    "        error_history.append(sum(grad**2))\n",
    "        if sum(grad**2) < error:\n",
    "            break\n",
    "        # max iteration\n",
    "        if n > max_iter:\n",
    "            print(\"    WARNING: max number of iterations\", max_iter)\n",
    "            break \n",
    "            \n",
    "    rewards = np.dot(feat_map, theta)\n",
    "    return rewards, policy, error_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 5/100\n",
      "iteration: 10/100\n",
      "iteration: 15/100\n",
      "iteration: 20/100\n",
      "iteration: 25/100\n",
      "iteration: 30/100\n",
      "iteration: 35/100\n",
      "iteration: 40/100\n"
     ]
    }
   ],
   "source": [
    "rewards, policy, error_history = maxent_irl(feat_map, P, trajs, 0.1, 0.01, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe33a0b9b10>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(error_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVcAAAEICAYAAADr6bc6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAASNUlEQVR4nO3ce9AddX3H8fdHEogFStB4wRDEC14prZgi1tYyo06Rqjhqp9ipgqPN9MLUttoRdYqt01ZtO2q9VAaVUayjjJeBaLEWqoCdFiTSyEVKjUzbpEmLgASiKAS//eNs6vHkPHmecPZ3zkl4v2Z2nt/u/s7+vtk8+3n27Nk9qSokSf160KwLkKT9keEqSQ0YrpLUgOEqSQ0YrpLUgOEqSQ0YrtIeJPlIkj+ddR3a9xiumpkk/5Hk7iQ7kvxPF2SHzLouqQ+Gq2bthVV1CPAzwNOAN86iiCTLZjGu9l+Gq+ZCVf0P8EUGIUuSg5L8VZL/SvK/Sc5J8uBu3eVJXtq1fz5JJTmlm39uko1d+3FJvpTktiS3Jvl4kpW7xuzOnN+Q5Frgu0mWJXlakmuS3JXkAmDFUP9VST6f5I4ktyf5ShKPIY3lL4bmQpIjgecDm7pF7wCewCBsHw+sBs7u1l0OnNS1nw3cDPzi0PzluzYLvA14FPBkYA3wxyNDvxz4ZWAlg+PhQuBjwEOATwEvHer7OmAL8DDgEcCbAJ8f11iGq2btwiR3AZuBW4C3JAnwG8DvV9XtVXUX8OfAad1rLufHw/RtQ/O/2K2nqjZV1SVV9YOq+jbwzqF+u7ynqjZX1d3AicBy4N1VdW9VfRq4eqjvvcARwKO79V8pv5xDCzBcNWsvrqpDGZyJPglYxeDM8CeAr3Vvwe8A/r5bDvAvwBOSPILBme35wJokq4ATgCsAkjw8ySeT/HeSO4G/7bY/bPNQ+1HAf48E5n8Otf+SwZn1PyS5OclZE/7btR8zXDUXqupy4CPAXwG3AncDT62qld10WPfBF1X1PeBrwGuB66vqHuCfgT8AvlVVt3abfRuDt+3HVdVPAr/O4FLBjw091N4GrO7OnHc5aqjGu6rqdVX1WOCFwB8keU4P/3zthwxXzZN3A88DjgM+CLwrycMBkqxO8ktDfS8HzuRH11cvG5kHOBTYAdyRZDXwh4uM/y/ATuB3uw+3XsLgTJiuhhckeXwXvncC93WTtBvDVXOjuy56PvBHwBsYvAW/sntLfynwxKHulzMIzysWmAf4E+B4YDvwd8BnFxn/HuAlwBnAd4BfHXnNMV0dOxgE8d9U1WV796/UA0W8Hi9J/fPMVZIamChckzwkySVJvtn9PHyBfvcl2dhN6ycZU5L2BRNdFkjyF8DtVfX27raUw6vqDWP67dj1Sa8kPRBMGq43ASdV1bYkRwCXVdUTx/QzXCU9oEwarndU1fCz2t+pqt0uDSTZCWxkcJvL26vqwgW2tw5YB3AAy55+8LKV47oJ4L4fzrqCueeHtXv2oCf6XTWL2X7Tt2+tqoct3nN3i+7dJJcCjxyz6s17Mc5RVbU1yWOBLyW5rqq+Ndqpqs4FzgU4bPnD6pkrX7IXQzyw1He/N+sS5t4Pv//9WZcw1w7+0P3KjAeUi37hA/+5eK/xFg3XqnruQuu6bys6YuiywC0LbGNr9/PmJJcx+Gq53cJVkvYXk96KtR44vWufDlw02iHJ4UkO6tqrgGcB35hwXEmaa5OG69uB5yX5JoPHFt8OkGRtkg91fZ4MbEjydeDLDK65Gq6S9msTXdGuqtuA3b64oqo2AK/p2v8M/NQk40jSvsYntCSpAcNVkhowXCWpAcNVkhowXCWpAcNVkhowXCWpAcNVkhowXCWpAcNVkhowXCWpAcNVkhowXCWpAcNVkhowXCWpAcNVkhowXCWpAcNVkhowXCWpAcNVkhowXCWpAcNVkhowXCWpAcNVkhowXCWpAcNVkhowXCWpgV7CNcnJSW5KsinJWWPWH5Tkgm79VUmO7mNcSZpXE4drkgOA9wPPB54CvDzJU0a6vRr4TlU9HngX8I5Jx5WkedbHmesJwKaqurmq7gE+CZw60udU4KNd+9PAc5Kkh7ElaS71Ea6rgc1D81u6ZWP7VNVOYDvw0B7GlqS5tKyHbYw7A6370Yck64B1ACsedMjklUnSjPRx5roFWDM0fySwdaE+SZYBhwG3j26oqs6tqrVVtfbAB63ooTRJmo0+wvVq4Jgkj0lyIHAasH6kz3rg9K79MuBLVbXbmask7S8mvixQVTuTnAl8ETgAOK+qbkjyVmBDVa0HPgx8LMkmBmesp006riTNsz6uuVJVFwMXjyw7e6j9feBX+hhLkvYFPqElSQ0YrpLUgOEqSQ0YrpLUgOEqSQ0YrpLUgOEqSQ0YrpLUgOEqSQ0YrpLUgOEqSQ0YrpLUgOEqSQ0YrpLUgOEqSQ0YrpLUgOEqSQ0YrpLUgOEqSQ0YrpLUgOEqSQ0YrpLUgOEqSQ0YrpLUgOEqSQ0YrpLUgOEqSQ0YrpLUQC/hmuTkJDcl2ZTkrDHrz0jy7SQbu+k1fYwrSfNq2aQbSHIA8H7gecAW4Ook66vqGyNdL6iqMycdT5L2BX2cuZ4AbKqqm6vqHuCTwKk9bFeS9lkTn7kCq4HNQ/NbgGeM6ffSJM8G/h34/araPNohyTpgHcBBBx3Gvcc+uofy9lNn3zrrCubeU1dum3UJc+1nD7lm1iXMvYsmeG0fZ64Zs6xG5j8HHF1VxwGXAh8dt6GqOreq1lbV2gOXH9xDaZI0G32E6xZgzdD8kcDW4Q5VdVtV/aCb/SDw9B7GlaS51Ue4Xg0ck+QxSQ4ETgPWD3dIcsTQ7IuAG3sYV5Lm1sTXXKtqZ5IzgS8CBwDnVdUNSd4KbKiq9cDvJnkRsBO4HThj0nElaZ718YEWVXUxcPHIsrOH2m8E3tjHWJK0L/AJLUlqwHCVpAYMV0lqwHCVpAYMV0lqwHCVpAYMV0lqwHCVpAYMV0lqwHCVpAYMV0lqwHCVpAYMV0lqwHCVpAYMV0lqwHCVpAYMV0lqwHCVpAYMV0lqwHCVpAYMV0lqwHCVpAYMV0lqwHCVpAYMV0lqwHCVpAYMV0lqoJdwTXJekluSXL/A+iR5T5JNSa5Ncnwf40rSvOrrzPUjwMl7WP984JhuWgd8oKdxJWku9RKuVXUFcPseupwKnF8DVwIrkxzRx9iSNI+mdc11NbB5aH5Lt+zHJFmXZEOSDffc+90plSZJ/ZtWuGbMstptQdW5VbW2qtYeuPzgKZQlSW1MK1y3AGuG5o8Etk5pbEmaummF63rgld1dAycC26tq25TGlqSpW9bHRpJ8AjgJWJVkC/AWYDlAVZ0DXAycAmwCvge8qo9xJWle9RKuVfXyRdYX8Dt9jCVJ+wKf0JKkBgxXSWrAcJWkBgxXSWrAcJWkBgxXSWrAcJWkBgxXSWrAcJWkBgxXSWrAcJWkBgxXSWrAcJWkBgxXSWrAcJWkBgxXSWrAcJWkBgxXSWrAcJWkBgxXSWrAcJWkBgxXSWrAcJWkBgxXSWrAcJWkBgxXSWrAcJWkBnoJ1yTnJbklyfULrD8pyfYkG7vp7D7GlaR5tayn7XwEeB9w/h76fKWqXtDTeJI013o5c62qK4Db+9iWJO0P+jpzXYpnJvk6sBV4fVXdMNohyTpgHcDyQw7nO8esmGJ5+5YLHv+JWZcw9+744YGzLmGuPf0g989iXjnBa6f1gdY1wKOr6qeB9wIXjutUVedW1dqqWrtsxcFTKk2S+jeVcK2qO6tqR9e+GFieZNU0xpakWZhKuCZ5ZJJ07RO6cW+bxtiSNAu9XHNN8gngJGBVki3AW4DlAFV1DvAy4LeS7ATuBk6rqupjbEmaR72Ea1W9fJH172Nwq5YkPSD4hJYkNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNTBxuCZZk+TLSW5MckOS147pkyTvSbIpybVJjp90XEmaZ8t62MZO4HVVdU2SQ4GvJbmkqr4x1Of5wDHd9AzgA91PSdovTXzmWlXbquqarn0XcCOweqTbqcD5NXAlsDLJEZOOLUnzqtdrrkmOBp4GXDWyajWweWh+C7sHsCTtN3oL1ySHAJ8Bfq+q7hxdPeYlNWYb65JsSLJh5/e/21dpkjR1vYRrkuUMgvXjVfXZMV22AGuG5o8Eto52qqpzq2ptVa1dtuLgPkqTpJno426BAB8Gbqyqdy7QbT3wyu6ugROB7VW1bdKxJWle9XG3wLOAVwDXJdnYLXsTcBRAVZ0DXAycAmwCvge8qodxJWluTRyuVfVPjL+mOtyngN+ZdCxJ2lf4hJYkNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDhqskNWC4SlIDE4drkjVJvpzkxiQ3JHntmD4nJdmeZGM3nT3puJI0z5b1sI2dwOuq6pokhwJfS3JJVX1jpN9XquoFPYwnSXNv4jPXqtpWVdd07buAG4HVk25XkvZlqar+NpYcDVwBHFtVdw4tPwn4DLAF2Aq8vqpuGPP6dcC6bvZY4PreiuvHKuDWWRcxxHr2bN7qgfmryXr27IlVdej9eWFv4ZrkEOBy4M+q6rMj634S+GFV7UhyCvDXVXXMItvbUFVreymuJ/NWk/Xs2bzVA/NXk/Xs2ST19HK3QJLlDM5MPz4arABVdWdV7ejaFwPLk6zqY2xJmkd93C0Q4MPAjVX1zgX6PLLrR5ITunFvm3RsSZpXfdwt8CzgFcB1STZ2y94EHAVQVecALwN+K8lO4G7gtFr8esS5PdTWt3mryXr2bN7qgfmryXr27H7X0+sHWpKkAZ/QkqQGDFdJamBuwjXJQ5JckuSb3c/DF+h339BjtOsb1HFykpuSbEpy1pj1ByW5oFt/VXdvb1NLqOmMJN8e2i+vaVjLeUluSTL2HuQMvKer9dokx7eqZS9qmtrj10t8HHyq+2jeHlFPsiLJV5N8vavnT8b0mdpxtsR69v4Yq6q5mIC/AM7q2mcB71ig346GNRwAfAt4LHAg8HXgKSN9fhs4p2ufBlzQeL8spaYzgPdN6f/p2cDxwPULrD8F+AIQ4ETgqjmo6STg81PaP0cAx3ftQ4F/H/P/NdV9tMSaprmPAhzStZcDVwEnjvSZ2nG2xHr2+hibmzNX4FTgo137o8CLZ1DDCcCmqrq5qu4BPtnVNWy4zk8Dz9l1m9kMa5qaqroCuH0PXU4Fzq+BK4GVSY6YcU1TU0t7HHyq+2iJNU1N9+/e0c0u76bRT9andpwtsZ69Nk/h+oiq2gaDXwbg4Qv0W5FkQ5Irk/QdwKuBzUPzW9j9l/D/+1TVTmA78NCe69jbmgBe2r3F/HSSNQ3rWcxS6522Z3Zv+76Q5KnTGLB7K/s0BmdCw2a2j/ZQE0xxHyU5oLt18xbgkqpacB9N4zhbQj2wl8fYVMM1yaVJrh8z7c2Z2FE1eBzt14B3J3lcnyWOWTb6F2wpffq0lPE+BxxdVccBl/Kjv/izMO39sxTXAI+uqp8G3gtc2HrADB4H/wzwezX0PRu7Vo95SfN9tEhNU91HVXVfVf0McCRwQpJjR8sd97IZ1rPXx9hUw7WqnltVx46ZLgL+d9dbo+7nLQtsY2v382bgMgZ/hfuyBRj+i3Qkgy+aGdsnyTLgMNq+JV20pqq6rap+0M1+EHh6w3oWs5R9OFU15cevs8jj4MxgHy1W07T30dC4dzA4jk8eWTXt42yP9dyfY2yeLgusB07v2qcDF412SHJ4koO69ioGT4eNfm/sJK4GjknymCQHMriQPnpHwnCdLwO+VN0V70YWrWnket2LGFxTm5X1wCu7T8RPBLbvutwzK5ni49fdOHt8HJwp76Ol1DTlffSwJCu79oOB5wL/NtJtasfZUuq5X8dYq0/g9nZicD3lH4Fvdj8f0i1fC3yoa/8ccB2DT8yvA17doI5TGHya+i3gzd2ytwIv6torgE8Bm4CvAo+dwr5ZrKa3ATd0++XLwJMa1vIJYBtwL4Ozi1cDvwn8Zv3ok9f3d7VeB6ydwv5ZrKYzh/bPlcDPNazl5xm8fb0W2NhNp8xyHy2xpmnuo+OAf+3quR44e8zv9NSOsyXWs9fHmI+/SlID83RZQJL2G4arJDVguEpSA4arJDVguEpSA4arJDVguEpSA/8HcpG3Ju17V5IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards_grid = rewards.reshape((g.length_max, g.height, g.width))\n",
    "rewards_grid = rewards_grid[0,:,:]\n",
    "\n",
    "# Map\n",
    "\n",
    "plt.imshow(rewards_grid);\n",
    "plt.title('Rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.18279359, -1.24136096, -1.29992832,  0.30321596],\n",
       "        [-0.88049824,  0.24372799,  0.42461965,  0.60551131],\n",
       "        [-0.57820289,  0.78548237,  0.726915  ,  0.90780666]],\n",
       "\n",
       "       [[-1.24969027, -1.30825764, -1.366825  ,  0.23631928],\n",
       "        [-0.94739492,  0.17683131,  0.35772297,  0.53861463],\n",
       "        [-0.64509957,  0.71858569,  0.66001832,  0.84090999]],\n",
       "\n",
       "       [[-1.31658695, -1.37515432, -1.43372168,  0.1694226 ],\n",
       "        [-1.0142916 ,  0.10993463,  0.29082629,  0.47171795],\n",
       "        [-0.71199625,  0.65168901,  0.59312165,  0.77401331]],\n",
       "\n",
       "       [[-1.38348363, -1.442051  , -1.50061836,  0.10252592],\n",
       "        [-1.08118828,  0.04303795,  0.22392961,  0.40482127],\n",
       "        [-0.77889293,  0.58479233,  0.52622497,  0.70711663]],\n",
       "\n",
       "       [[-1.45038031, -1.50894768, -1.56751504,  0.03562924],\n",
       "        [-1.14808496, -0.02385873,  0.15703293,  0.33792459],\n",
       "        [-0.84578961,  0.51789565,  0.45932829,  0.64021995]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards.reshape((g.length_max, g.height, g.width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_demonstrations(g, policy, n_trajs=10, len_traj=5):\n",
    "    \"\"\"gatheres expert demonstrations\n",
    "    inputs:\n",
    "    policy      Nx1 matrix\n",
    "    n_trajs     int - number of trajectories to generate\n",
    "    rand_start  bool - randomly picking start position or not\n",
    "    start_pos   2x1 list - set start position, default [0,0]\n",
    "    returns:\n",
    "    trajs       a list of trajectories - each element in the list is a list of Steps representing an episode\n",
    "    \"\"\"\n",
    "\n",
    "    trajs = []\n",
    "    for i in range(n_trajs):\n",
    "        \n",
    "        episode = []\n",
    "        state = (0,1,1)\n",
    "        idx = g.state2idx(state)\n",
    "        episode.append(idx)\n",
    "        \n",
    "        # while not is_done:\n",
    "        for _ in range(len_traj-1):\n",
    "\n",
    "            act = np.random.choice(g.n_actions, p= policy[idx,:])\n",
    "            next_state = g.get_next_state(state, act)\n",
    "            next_idx = g.state2idx(next_state)\n",
    "            episode.append(next_idx)\n",
    "            state = next_state\n",
    "            idx = next_idx\n",
    "            \n",
    "        trajs.append(episode)\n",
    "    return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 21, 29, 42, 53],\n",
       " [5, 21, 33, 45, 53],\n",
       " [5, 22, 34, 46, 53],\n",
       " [5, 22, 34, 46, 53],\n",
       " [5, 21, 33, 42, 53],\n",
       " [5, 18, 34, 46, 53],\n",
       " [5, 18, 34, 45, 53],\n",
       " [5, 18, 33, 45, 53],\n",
       " [5, 16, 28, 41, 53],\n",
       " [5, 18, 35, 42, 53],\n",
       " [5, 21, 33, 45, 53],\n",
       " [5, 22, 34, 46, 53],\n",
       " [5, 21, 33, 45, 53],\n",
       " [5, 22, 35, 42, 53],\n",
       " [5, 17, 29, 45, 53],\n",
       " [5, 17, 33, 45, 53],\n",
       " [5, 22, 34, 46, 53],\n",
       " [5, 21, 33, 42, 53],\n",
       " [5, 22, 34, 45, 53],\n",
       " [5, 21, 33, 45, 53],\n",
       " [5, 17, 33, 45, 53],\n",
       " [5, 18, 30, 45, 53],\n",
       " [5, 18, 33, 42, 53],\n",
       " [5, 21, 33, 41, 53],\n",
       " [5, 22, 33, 41, 53],\n",
       " [5, 18, 27, 42, 53],\n",
       " [5, 22, 34, 46, 53],\n",
       " [5, 22, 33, 45, 53],\n",
       " [5, 21, 34, 46, 53],\n",
       " [5, 21, 33, 46, 53],\n",
       " [5, 22, 34, 46, 53],\n",
       " [5, 21, 33, 45, 53],\n",
       " [5, 21, 33, 45, 53],\n",
       " [5, 22, 34, 46, 53],\n",
       " [5, 18, 35, 46, 53],\n",
       " [5, 22, 34, 45, 53],\n",
       " [5, 18, 35, 46, 53],\n",
       " [5, 22, 31, 46, 53],\n",
       " [5, 21, 33, 45, 53],\n",
       " [5, 20, 33, 42, 53],\n",
       " [5, 18, 30, 46, 53],\n",
       " [5, 17, 30, 46, 53],\n",
       " [5, 21, 33, 45, 53],\n",
       " [5, 21, 33, 45, 53],\n",
       " [5, 21, 29, 42, 53],\n",
       " [5, 22, 34, 46, 53],\n",
       " [5, 22, 30, 46, 53],\n",
       " [5, 22, 34, 46, 53],\n",
       " [5, 18, 34, 46, 53],\n",
       " [5, 18, 31, 46, 53]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajs = generate_demonstrations(g, policy, 50, 5)\n",
    "trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,   1.],\n",
       "       [  2., 110.,  25.,   2.],\n",
       "       [  1.,  54.,  51.,   4.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = np.zeros((3,4))\n",
    "for traj in trajs:\n",
    "    for idx in traj:\n",
    "        state = g.idx2state(idx)\n",
    "        freq[state[1], state[2]] += 1\n",
    "        \n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep\n",
    "\n",
    "Deep MaxEnt Inverse reinforcement Learning - the reward function is a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-bd371dba3403>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mConvNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConvNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         self.layer1 = nn.Sequential(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepIRLFC:\n",
    "\n",
    "\n",
    "    def __init__(self, n_input, lr, n_h1=400, n_h2=300, l2=10, name='deep_irl_fc'):\n",
    "        self.n_input = n_input\n",
    "        self.lr = lr\n",
    "        self.n_h1 = n_h1\n",
    "        self.n_h2 = n_h2\n",
    "        self.name = name\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.input_s, self.reward, self.theta = self._build_network(self.name)\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "\n",
    "        self.grad_r = tf.placeholder(tf.float32, [None, 1])\n",
    "        self.l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in self.theta])\n",
    "        self.grad_l2 = tf.gradients(self.l2_loss, self.theta)\n",
    "\n",
    "        self.grad_theta = tf.gradients(self.reward, self.theta, -self.grad_r)\n",
    "        # apply l2 loss gradients\n",
    "        self.grad_theta = [tf.add(l2*self.grad_l2[i], self.grad_theta[i]) for i in range(len(self.grad_l2))]\n",
    "        self.grad_theta, _ = tf.clip_by_global_norm(self.grad_theta, 100.0)\n",
    "\n",
    "        self.grad_norms = tf.global_norm(self.grad_theta)\n",
    "        self.optimize = self.optimizer.apply_gradients(zip(self.grad_theta, self.theta))\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    def _build_network(self, name):\n",
    "        input_s = tf.placeholder(tf.float32, [None, self.n_input])\n",
    "        with tf.variable_scope(name):\n",
    "            fc1 = fc(input_s, self.n_h1, scope=\"fc1\", activation_fn=tf.nn.elu,\n",
    "            initializer=tf.contrib.layers.variance_scaling_initializer(mode=\"FAN_IN\"))\n",
    "            fc2 = fc(fc1, self.n_h2, scope=\"fc2\", activation_fn=tf.nn.elu,\n",
    "            initializer=tf.contrib.layers.variance_scaling_initializer(mode=\"FAN_IN\"))\n",
    "            reward = fc(fc2, 1, scope=\"reward\")\n",
    "        theta = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
    "        return input_s, reward, theta\n",
    "\n",
    "\n",
    "    def get_theta(self):\n",
    "        return self.sess.run(self.theta)\n",
    "\n",
    "\n",
    "    def get_rewards(self, states):\n",
    "        rewards = self.sess.run(self.reward, feed_dict={self.input_s: states})\n",
    "        return rewards\n",
    "\n",
    "\n",
    "    def apply_grads(self, feat_map, grad_r):\n",
    "        grad_r = np.reshape(grad_r, [-1, 1])\n",
    "        feat_map = np.reshape(feat_map, [-1, self.n_input])\n",
    "        _, grad_theta, l2_loss, grad_norms = self.sess.run([self.optimize, self.grad_theta, self.l2_loss, self.grad_norms], \n",
    "          feed_dict={self.grad_r: grad_r, self.input_s: feat_map})\n",
    "        return grad_theta, l2_loss, grad_norms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_svf(trajs, n_states):\n",
    "    \"\"\"\n",
    "    compute state visitation frequences from demonstrations\n",
    "\n",
    "    input:\n",
    "    trajs   list of list of Steps - collected from expert\n",
    "    returns:\n",
    "    p       Nx1 vector - state visitation frequences   \n",
    "    \"\"\"\n",
    "\n",
    "    p = np.zeros(n_states)\n",
    "    for traj in trajs:\n",
    "        for step in traj:\n",
    "            p[step] += 1\n",
    "    p = p/len(trajs)\n",
    "    return p\n",
    "\n",
    "\n",
    "def deep_maxent_irl(feat_map, P_a, trajs, lr, n_iters):\n",
    "    \"\"\"\n",
    "    Maximum Entropy Inverse Reinforcement Learning (Maxent IRL)\n",
    "    inputs:\n",
    "    feat_map    NxD matrix - the features for each state\n",
    "    P_a         NxNxN_ACTIONS matrix - P_a[s0, s1, a] is the transition prob of \n",
    "                                   landing at state s1 when taking action \n",
    "                                   a at state s0\n",
    "    gamma       float - RL discount factor\n",
    "    trajs       a list of demonstrations\n",
    "    lr          float - learning rate\n",
    "    n_iters     int - number of optimization steps\n",
    "    returns\n",
    "    rewards     Nx1 vector - recoverred state rewards\n",
    "    \"\"\"\n",
    "\n",
    "    # tf.set_random_seed(1)\n",
    "\n",
    "    N_STATES, _, N_ACTIONS = np.shape(P_a)\n",
    "\n",
    "    # init nn model\n",
    "    nn_r = DeepIRLFC(feat_map.shape[1], lr, 3, 3)\n",
    "\n",
    "    # find state visitation frequencies using demonstrations\n",
    "    mu_D = demo_svf(trajs, N_STATES)\n",
    "\n",
    "    # training \n",
    "    for iteration in range(n_iters):\n",
    "        if iteration % (n_iters/10) == 0:\n",
    "            print('iteration: {}'.format(iteration))\n",
    "\n",
    "        # compute the reward matrix\n",
    "        rewards = nn_r.get_rewards(feat_map)\n",
    "\n",
    "        # compute policy \n",
    "        _, policy = value_iteration(P_a, rewards, error=0.01, max_iter=100)\n",
    "\n",
    "        # compute expected svf\n",
    "        mu_exp = compute_state_visition_freq(P_a, g.state2idx((0,1,1)), g.length_max, policy)\n",
    "\n",
    "        # compute gradients on rewards:\n",
    "        grad_r = mu_D - mu_exp\n",
    "\n",
    "        # apply gradients to the neural network\n",
    "        grad_theta, l2_loss, grad_norm = nn_r.apply_grads(feat_map, grad_r)\n",
    "\n",
    "\n",
    "    rewards = nn_r.get_rewards(feat_map)\n",
    "    # return sigmoid(normalize(rewards))\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = deep_maxent_irl(feat_map, P, trajs, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

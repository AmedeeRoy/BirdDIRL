{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRL\n",
    "\n",
    "Easy game formalism :\n",
    "\n",
    "- States = (x, t)\n",
    "- Action = (&uarr;, &darr;, &rarr;, &larr;)\n",
    "- Reward = r(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridworld provides a basic environment for RL agents to interact with\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    Grid world environment\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, length_max, height, width, start_pos):\n",
    "        \"\"\"\n",
    "            input: \n",
    "            height - idx : height of the spatial grid\n",
    "            width - idx : width of the spatial grid\n",
    "            length - idx : temporal length of a trip\n",
    "            \n",
    "            start_pos 2-tuple : coordinates within the state_space (height x width)\n",
    "            \n",
    "        \"\"\"\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.length_max = length_max\n",
    "        \n",
    "        self.start = (0, start_pos[0], start_pos[1])\n",
    "        self.end = (length_max-1, start_pos[0], start_pos[1])\n",
    "        \n",
    "        self.n_states = self.height*self.width*self.length_max\n",
    "        \n",
    "        self.actions = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.neighbors = [(0, 0),(-1, 0),(-1, 1),(0, 1),(1, 1),(1, 0),(1, -1),(0, -1),(-1, -1)]\n",
    "        self.dirs = {0: 'stay', 1: 'n', 2: 'ne', 3: 'e', 4: 'se', 5: 's', 6: 'sw', 7: 'w', 8: 'nw'}\n",
    "    \n",
    "    def get_grid_idx(self):\n",
    "        return np.array(range(self.n_states)).reshape((self.length_max, self.height, self.width))\n",
    "    \n",
    "    def get_list_state(self):\n",
    "        return [(i,j,k) for i in range(self.length_max) for j in range(self.height) for k in range(self.width)]\n",
    "    \n",
    "    def state2idx(self, state):\n",
    "        \"\"\"\n",
    "        input:\n",
    "          2d state\n",
    "        returns:\n",
    "          1d index\n",
    "        \"\"\"\n",
    "        return self.get_grid_idx()[state]\n",
    "\n",
    "    def idx2state(self, idx):\n",
    "        \"\"\"\n",
    "        input:\n",
    "          1d idx\n",
    "        returns:\n",
    "          2d state\n",
    "        \"\"\"\n",
    "        return self.get_list_state()[idx]\n",
    "           \n",
    "    def get_next_state(self, state, action):\n",
    "        \"\"\"\n",
    "        get next state with [action] on [state]\n",
    "        args\n",
    "          state     (z, y, x)\n",
    "          action    int\n",
    "        returns\n",
    "          new state\n",
    "        \"\"\"\n",
    "        if state[0] >= self.length_max-1:\n",
    "            return state\n",
    "        else :\n",
    "            inc = self.neighbors[action]\n",
    "            nei_s = (state[1] + inc[0], state[2] + inc[1])\n",
    "            if nei_s[0] >= 0 and nei_s[0] < self.height and nei_s[1] >= 0 and nei_s[1] < self.width:\n",
    "                next_state = (state[0] + 1, nei_s[0], nei_s[1])\n",
    "            else:\n",
    "                next_state = (state[0] + 1, state[1], state[2])\n",
    "            return next_state\n",
    "        \n",
    "    def get_transition_mat(self):\n",
    "        \"\"\"\n",
    "        get transition dynamics of the gridworld\n",
    "        return:\n",
    "          P_a         NxNxN_ACTIONS transition probabilities matrix - \n",
    "                        P_a[s0, s1, a] is the transition prob of \n",
    "                        landing at state s1 when taking action \n",
    "                        a at state s0\n",
    "        \"\"\"\n",
    "        P_a = np.zeros((self.n_states, self.n_states, self.n_actions))\n",
    "        \n",
    "        for i in range(self.n_states):\n",
    "            si = self.idx2state(i)\n",
    "            for a in range(self.n_actions):\n",
    "                sj = self.get_next_state(si,a)\n",
    "                j = self.state2idx(sj)\n",
    "                P_a[i, j, a] = 1                \n",
    "        return P_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = GridWorld(5,3,4,(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]],\n",
       "\n",
       "       [[12, 13, 14, 15],\n",
       "        [16, 17, 18, 19],\n",
       "        [20, 21, 22, 23]],\n",
       "\n",
       "       [[24, 25, 26, 27],\n",
       "        [28, 29, 30, 31],\n",
       "        [32, 33, 34, 35]],\n",
       "\n",
       "       [[36, 37, 38, 39],\n",
       "        [40, 41, 42, 43],\n",
       "        [44, 45, 46, 47]],\n",
       "\n",
       "       [[48, 49, 50, 51],\n",
       "        [52, 53, 54, 55],\n",
       "        [56, 57, 58, 59]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.get_grid_idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P_a, rewards, gamma, error=0.01, deterministic=True):\n",
    "    \"\"\"\n",
    "    static value iteration function. Perhaps the most useful function in this repo\n",
    "\n",
    "    inputs:\n",
    "    P_a         NxNxN_ACTIONS transition probabilities matrix - \n",
    "                          P_a[s0, s1, a] is the transition prob of \n",
    "                          landing at state s1 when taking action \n",
    "                          a at state s0\n",
    "    rewards     Nx1 matrix - rewards for all the states\n",
    "    gamma       float - RL discount\n",
    "    error       float - threshold for a stop\n",
    "\n",
    "    returns:\n",
    "    values    Nx1 matrix - estimated values\n",
    "    policy    Nx1 matrix - policy\n",
    "    \"\"\"\n",
    "    N_STATES, _, N_ACTIONS = np.shape(P_a)\n",
    "\n",
    "    values = np.zeros([N_STATES])\n",
    "\n",
    "    # estimate values\n",
    "    while True:\n",
    "        values_tmp = values.copy()\n",
    "\n",
    "        for s in range(N_STATES-1):\n",
    "            v_s = []\n",
    "            values[s] = max([sum([P_a[s, s1, a]*(rewards[s] + gamma*values_tmp[s1]) for s1 in range(N_STATES)]) for a in range(N_ACTIONS)])\n",
    "\n",
    "        if max([abs(values[s] - values_tmp[s]) for s in range(N_STATES)]) < error:\n",
    "            break\n",
    "\n",
    "    if deterministic:\n",
    "        policy = np.zeros([N_STATES])\n",
    "        for s in range(N_STATES):\n",
    "            policy[s] = np.argmax([sum([P_a[s, s1, a]*(rewards[s]+gamma*values[s1]) \n",
    "                                      for s1 in range(N_STATES)]) \n",
    "                                      for a in range(N_ACTIONS)])\n",
    "        return values, policy\n",
    "    else:\n",
    "        # generate stochastic policy\n",
    "        policy = np.zeros([N_STATES, N_ACTIONS])\n",
    "        for s in range(N_STATES):\n",
    "            v_s = np.array([sum([P_a[s, s1, a]*(rewards[s] + gamma*values[s1]) for s1 in range(N_STATES)]) for a in range(N_ACTIONS)])\n",
    "            policy[s,:] = np.transpose(v_s/np.sum(v_s))\n",
    "        return values, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-baff4cced975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mP_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transition_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7aa9eaae63d3>\u001b[0m in \u001b[0;36mvalue_iteration\u001b[0;34m(P_a, rewards, gamma, error, deterministic)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_STATES\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mv_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mP_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_STATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_ACTIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_STATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7aa9eaae63d3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_STATES\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mv_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mP_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_STATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_ACTIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_STATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7aa9eaae63d3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_STATES\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mv_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mP_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_STATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_ACTIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_STATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "P_a = g.get_transition_mat()\n",
    "rewards = np.random.random((60,1))\n",
    "v, p = value_iteration(P_a, rewards, 0.99, 0.01, deterministic = False)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_state_visition_freq(P_a, gamma, start_idx, nb_step, policy, deterministic=True):\n",
    "    \"\"\"compute the expected states visition frequency p(s| theta, T) \n",
    "    using dynamic programming\n",
    "    inputs:\n",
    "    P_a     NxNxN_ACTIONS matrix - transition dynamics\n",
    "    gamma   float - discount factor\n",
    "    start_idx   idx of start position\n",
    "    nb_step idx - nb of step to iterate\n",
    "    policy  Nx1 vector - policy\n",
    "\n",
    "    returns:\n",
    "    p       Nx1 vector - state visitation frequencies\n",
    "    \"\"\"\n",
    "    N_STATES, _, N_ACTIONS = np.shape(P_a)\n",
    "\n",
    "    # mu[s, t] is the prob of visiting state s at time t\n",
    "    mu = np.zeros([N_STATES, nb_step]) \n",
    "\n",
    "    mu[start_idx, 0] = 1\n",
    "    for s in range(N_STATES):\n",
    "        for t in range(nb_step-1):\n",
    "            if deterministic:\n",
    "                mu[s, t+1] = sum([mu[pre_s, t]*P_a[pre_s, s, int(policy[pre_s])] for pre_s in range(N_STATES)])\n",
    "            else:\n",
    "                mu[s, t+1] = sum([sum([mu[pre_s, t]*P_a[pre_s, s, a1]*policy[pre_s, a1] for a1 in range(N_ACTIONS)]) for pre_s in range(N_STATES)])\n",
    "\n",
    "    p = np.sum(mu, 1)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.11706153e-01, 1.10515772e-01, 1.11713626e-01, 0.00000000e+00,\n",
       "       1.10545281e-01, 1.11739382e-01, 1.10966790e-01, 0.00000000e+00,\n",
       "       1.11391502e-01, 1.10538601e-01, 1.10882893e-01, 0.00000000e+00,\n",
       "       1.11003447e-01, 1.13030853e-01, 9.00931058e-02, 2.03600122e-02,\n",
       "       1.10933185e-01, 1.13866697e-01, 7.64801052e-02, 3.02734044e-02,\n",
       "       1.11125898e-01, 1.12435387e-01, 9.01202489e-02, 2.02776559e-02,\n",
       "       1.03567370e-01, 9.86793846e-02, 7.58171727e-02, 3.51917934e-02,\n",
       "       1.22196087e-01, 1.20957892e-01, 7.07778465e-02, 4.80545486e-02,\n",
       "       1.15188131e-01, 1.17837337e-01, 6.53762134e-02, 2.63562240e-02,\n",
       "       7.45600246e-02, 6.50001718e-02, 1.01585288e-01, 5.77566862e-02,\n",
       "       1.53087025e-01, 1.02276987e-01, 6.92460154e-02, 6.16885270e-02,\n",
       "       1.52051359e-01, 1.14167736e-01, 4.81697395e-02, 4.10440568e-04])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svf = compute_state_visition_freq(P_a, 0.99, g.state2idx((0,1,1)), g.length_max, policy, deterministic = False)\n",
    "svf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]],\n",
       "\n",
       "       [[12, 13, 14, 15],\n",
       "        [16, 17, 18, 19],\n",
       "        [20, 21, 22, 23]],\n",
       "\n",
       "       [[24, 25, 26, 27],\n",
       "        [28, 29, 30, 31],\n",
       "        [32, 33, 34, 35]],\n",
       "\n",
       "       [[36, 37, 38, 39],\n",
       "        [40, 41, 42, 43],\n",
       "        [44, 45, 46, 47]],\n",
       "\n",
       "       [[48, 49, 50, 51],\n",
       "        [52, 53, 54, 55],\n",
       "        [56, 57, 58, 59]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.get_grid_idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create expert trajectories\n",
    "\n",
    "traj_1 = [5, 18, 33, 46, 53]\n",
    "traj_2 = [5, 22, 30, 45, 53]\n",
    "traj_3 = [5, 21, 34, 45, 53]\n",
    "traj_4 = [5, 21, 33, 45, 53]\n",
    "traj_5 = [5, 22, 34, 42, 53]\n",
    "traj_6 = [5, 22, 34, 42, 53]\n",
    "\n",
    "trajs = [traj_1, traj_2, traj_3, traj_4, traj_5, traj_6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feat_map\n",
    "\n",
    "map_mask = [[-1, -1, -1, -1],\n",
    "            [-1, 0, 0, -1],\n",
    "            [-1, 0, 0, -1]]\n",
    "\n",
    "map_dist = [[-0.5, -0.5, -0.5, -1],\n",
    "            [-0.5, 0, -0.5, -1],\n",
    "            [-0.5, -0.5, -0.5, -1]]\n",
    "\n",
    "map_gradv = [[0, 0, 0, 0],\n",
    "            [-0.5, -0.5, -0.5, -0.5],\n",
    "            [-1, -1, -1, -1]]\n",
    "\n",
    "map_gradh = [[0, -0.3, -0.6, -0.9],\n",
    "            [0, -0.3, -0.6, -0.9],\n",
    "            [0, -0.3, -0.6, -0.9]]\n",
    "\n",
    "map_const = [[0, 0, 0, 0],\n",
    "            [0, 0, 0, 0],\n",
    "            [0, 0, 0, 0]]\n",
    "\n",
    "\n",
    "\n",
    "feat_map = np.array([map_mask, map_dist, map_gradv, map_gradh, map_const]).reshape(5,12).T\n",
    "for t in range(g.length_max-1):\n",
    "    feature =  np.array([map_mask, map_dist, map_gradv, map_gradh, map_const]).reshape(5,12).T\n",
    "    feature[:,4] += 1\n",
    "    \n",
    "    feat_map = np.vstack([feat_map, feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxent_irl(feat_map, P_a, gamma, trajs, lr, n_iters):\n",
    "    \"\"\"\n",
    "    Maximum Entropy Inverse Reinforcement Learning (Maxent IRL)\n",
    "    inputs:\n",
    "    feat_map    NxD matrix - the features for each state\n",
    "    P_a         NxNxN_ACTIONS matrix - P_a[s0, s1, a] is the transition prob of \n",
    "                                       landing at state s1 when taking action \n",
    "                                       a at state s0\n",
    "    gamma       float - RL discount factor\n",
    "    trajs       a list of demonstrations\n",
    "    lr          float - learning rate\n",
    "    n_iters     int - number of optimization steps\n",
    "    returns\n",
    "    rewards     Nx1 vector - recoverred state rewards\n",
    "    \"\"\"\n",
    "    N_STATES, _, N_ACTIONS = np.shape(P_a)\n",
    "\n",
    "    # init parameters\n",
    "    theta = np.random.uniform(size=(feat_map.shape[1]))\n",
    "\n",
    "    # calc feature expectations\n",
    "    feat_exp = np.zeros([feat_map.shape[1]])\n",
    "    for episode in trajs:\n",
    "        for step in episode:\n",
    "            feat_exp += feat_map[step,:]\n",
    "    feat_exp = feat_exp/len(trajs)\n",
    "\n",
    "    # training\n",
    "    for iteration in range(n_iters):\n",
    "\n",
    "        if iteration % (n_iters/20) == 0:\n",
    "            print('iteration: {}/{}'.format(iteration, n_iters))\n",
    "            print(theta)\n",
    "\n",
    "        # compute reward function\n",
    "        rewards = np.dot(feat_map, theta)\n",
    "\n",
    "        # compute policy\n",
    "        _, policy = value_iteration(P_a, rewards, gamma, error=0.01, deterministic = False)\n",
    "\n",
    "        # compute state visition frequences\n",
    "        svf = compute_state_visition_freq(P_a, gamma, 5, 10, policy, deterministic = False)\n",
    "        \n",
    "        # compute gradients\n",
    "        grad = feat_exp - feat_map.T.dot(svf)\n",
    "\n",
    "        # update params\n",
    "        theta += lr * grad\n",
    "\n",
    "    rewards = np.dot(feat_map, theta)\n",
    "    return rewards, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0/10\n",
      "[0.72270639 0.74017267 0.18466445 0.4669599  0.76782433]\n",
      "iteration: 1/10\n",
      "[0.55770639 0.64267267 0.32016445 0.5629599  0.51782433]\n",
      "iteration: 2/10\n",
      "[0.39270639 0.54517267 0.45566445 0.6589599  0.26782433]\n",
      "iteration: 3/10\n",
      "[0.52770639 0.77267267 0.74116445 0.9499599  0.01782433]\n",
      "iteration: 4/10\n",
      "[ 0.66270639  1.00017267  1.02666445  1.2409599  -0.23217567]\n",
      "iteration: 5/10\n",
      "[ 0.79770639  1.22767267  1.31216445  1.5319599  -0.48217567]\n",
      "iteration: 6/10\n",
      "[ 0.93270639  1.45517267  1.59766445  1.8229599  -0.73217567]\n",
      "iteration: 7/10\n",
      "[ 1.06770639  1.68267267  1.88316445  2.1139599  -0.98217567]\n",
      "iteration: 8/10\n",
      "[ 1.20270639  1.91017267  2.16866445  2.4049599  -1.23217567]\n",
      "iteration: 9/10\n",
      "[ 1.33770639  2.13767267  2.45416445  2.6959599  -1.48217567]\n"
     ]
    }
   ],
   "source": [
    "rewards, policy = maxent_irl(feat_map, P_a, 0.99, trajs, 0.05, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.95540887, 0.85278845, 0.75016803, 0.51211726],\n",
       "        [0.79853503, 1.        , 0.76194923, 0.35524342],\n",
       "        [0.6416612 , 0.70769581, 0.60507539, 0.19836958]],\n",
       "\n",
       "       [[0.75703929, 0.65441887, 0.55179845, 0.31374768],\n",
       "        [0.60016545, 0.80163042, 0.56357965, 0.15687384],\n",
       "        [0.44329161, 0.50932623, 0.40670581, 0.        ]],\n",
       "\n",
       "       [[0.75703929, 0.65441887, 0.55179845, 0.31374768],\n",
       "        [0.60016545, 0.80163042, 0.56357965, 0.15687384],\n",
       "        [0.44329161, 0.50932623, 0.40670581, 0.        ]],\n",
       "\n",
       "       [[0.75703929, 0.65441887, 0.55179845, 0.31374768],\n",
       "        [0.60016545, 0.80163042, 0.56357965, 0.15687384],\n",
       "        [0.44329161, 0.50932623, 0.40670581, 0.        ]],\n",
       "\n",
       "       [[0.75703929, 0.65441887, 0.55179845, 0.31374768],\n",
       "        [0.60016545, 0.80163042, 0.56357965, 0.15687384],\n",
       "        [0.44329161, 0.50932623, 0.40670581, 0.        ]]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards.reshape((g.length_max, g.height, g.width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards_grid = rewards.reshape((g.length_max, g.height, g.width))\n",
    "rewards_grid = rewards_grid[0,:,:]\n",
    "\n",
    "# Map\n",
    "\n",
    "plt.imshow(rewards_grid);\n",
    "plt.title('Rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_demonstrations(g, policy, n_trajs=10, len_traj=5):\n",
    "    \"\"\"gatheres expert demonstrations\n",
    "    inputs:\n",
    "    policy      Nx1 matrix\n",
    "    n_trajs     int - number of trajectories to generate\n",
    "    rand_start  bool - randomly picking start position or not\n",
    "    start_pos   2x1 list - set start position, default [0,0]\n",
    "    returns:\n",
    "    trajs       a list of trajectories - each element in the list is a list of Steps representing an episode\n",
    "    \"\"\"\n",
    "\n",
    "    trajs = []\n",
    "    for i in range(n_trajs):\n",
    "        \n",
    "        episode = []\n",
    "        state = (0,1,1)\n",
    "        idx = g.state2idx(state)\n",
    "        episode.append(idx)\n",
    "        \n",
    "        # while not is_done:\n",
    "        for _ in range(len_traj-1):\n",
    "\n",
    "#             act = np.random.choice(g.n_actions, p= policy[idx,:])\n",
    "            act = int(policy[idx])\n",
    "            next_state = g.get_next_state(state, act)\n",
    "            next_idx = g.state2idx(next_state)\n",
    "            episode.append(next_idx)\n",
    "            state = next_state\n",
    "            idx = next_idx\n",
    "            \n",
    "        trajs.append(episode)\n",
    "    return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 5., 6., 6., 3., 0., 7., 7., 2., 1., 8., 8., 4., 5., 6., 6., 3.,\n",
       "       0., 7., 7., 2., 1., 8., 8., 4., 4., 5., 6., 3., 3., 0., 7., 2., 2.,\n",
       "       1., 8., 4., 5., 6., 6., 3., 0., 4., 5., 2., 1., 3., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59],\n",
       " [5, 17, 29, 42, 59]]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajs = generate_demonstrations(g, policy, 50, 5)\n",
    "trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,   0.],\n",
       "       [  0., 150.,  50.,   0.],\n",
       "       [  0.,   0.,   0.,  50.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = np.zeros((3,4))\n",
    "for traj in trajs:\n",
    "    for idx in traj:\n",
    "        state = g.idx2state(idx)\n",
    "        freq[state[1], state[2]] += 1\n",
    "        \n",
    "freq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
